---
title: "Integer sample size and event counts"
author: "Keaven Anderson"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: gsDesign.bib

vignette: >
  %\VignetteIndexEntry{Integer sample size and event counts}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Introduction 

The **gsDesign** package was originally designed to have continuous sample size planned.
Designs with time-to-event outcomes also had non-integer event counts at times of analysis.
This vignette  documents the capability to convert to integer sample sizes and event counts.
This has a couple of implications on design characteristics:

- Information fraction on output will not be exactly as input due to rounding.
- Power on output will not be exactly as input.

This document goes through a variety examples to demonstrate the calculations done and their accuracy.
The new function as of June, 2023 is the `toInteger()` which operates on group sequential designs to conver to integer-based sample size and event counts at analyses.

# Binomial endpoint designs

## Fixed sample size

We begin with as simple example based on comparing two binomial rates with interim analyses after 50% and 75% of events. We assume a 2:1 experimental:control randomization ratio. Note that sample size is not an integer.

```{r}
library(gsDesign)
n.fix <- nBinomial(p1 = .2, p2 = .1, alpha = .025, beta = .2, ratio = 2)
n.fix
```

## 1-sided design

Now we convert this to a 1-sided group sequential design  with interims after 50% and 75% of observations. Again, sample size at each analysis is not an integer. We use the Lan-DeMets spending function approximating an O'Brien-Fleming efficacy bound.

```{r}
x <- gsDesign(alpha = .025, beta = .2, n.fix = n.fix, test.type = 1, sfu = sfLDOF, timing = c(.5, .75))
x$n.I
```
Now we convert to integer sample sizes at each analysis. Interim sample sizes are rounded to the nearest integer.
The default `roundUpFinal = TRUE` rounds the final sample size to the nearest integer to 1 + the experimental:control randomization ratio. Thus, the final sample size of 441 below is a multiple of 3.

```{r}
x_integer <- toInteger(x, ratio = 2)
x_integer$n.I
```

Next we examine the efficacy bound of the 2 designs.

```{r}
x$upper$bound
x_integer$upper$bound
```

The differences are associated with slightly different timing of the analyses associated with the different sample sizes noted above:

```{r}
x$timing
x_integer$timing
```

These differences also make a difference in the Type I error associated with each analysis

```{r}
x$upper$prob
x_integer$upper$prob
```

Cumulative probabilities of the first column above correspond to cumulative spending for each of these:

```{r}
cumsum(x$upper$prob[,1])
# Specified spending
x$upper$sf(alpha = x$alpha, t = x$timing, x$upper$param)$spend
```
Again, this deviates a bit from the integer-based design:

```{r}
cumsum(x_integer$upper$prob[,1])
# Specified spending
x$upper$sf(alpha = x_integer$alpha, t = x_integer$timing, x_integer$upper$param)$spend
```
Finally, we look at cumulative boundary crossing probabilities under the alternate hypothesis for each design.
Due to rounding up the final sample size, the integer-based design has slightly higher total power than the specified 80% (Type II erorr `beta = 0.2.`
Interim power is slightly lower for the integer-based design since sample size is rounded to the nearest integer rather than rounded up as at the final analysis.

```{r}
cumsum(x$upper$prob[,2])
cumsum(x_integer$upper$prob[,2])
```

## Non-binding design

The default `test.type = 4` has a non-binding futility bound. We examine behavior of this design next.
The futility bound is moderately aggresssive and, thus, there is a compensatory increase in sample size to retain power.
The parameter `delta` is the natural parameter denoting the difference in response (or failure) rates of 0.2 vs. 0.1 that was specified in the call to `nBinomial()` above.


```{r}
xnb <- gsDesign(alpha = .025, beta = .2, n.fix = n.fix, test.type = 4, 
                sfu = sfLDOF, sfl = sfHSD, sflpar = -2, 
                timing = c(.5, .75), delta1 = .1)
xnb$n.I
x$n.I
```

Now we repeat the exercises above.
First, we convert to integer sample sizes at each analysis. Interim sample sizes are rounded to the nearest integer.
The default `roundUpFinal = TRUE` rounds the final sample size to the nearest integer to 1 + the experimental:control randomization ratio. Thus, the final sample size of 441 below is a multiple of 3.

```{r}
xnbi <- toInteger(xnb, ratio = 2)
xnbi$n.I
```

Next we examine the efficacy bound of the 2 designs.

```{r}
xnb$upper$bound
xnbi$upper$bound
```

The differences are associated with slightly different timing of the analyses for the different sample sizes noted above:

```{r}
xnb$timing
xnbi$timing
```

These differences also make a difference in the Type I error associated with each analysis

```{r}
x$upper$prob
x_integer$upper$prob
```

Cumulative probabilities of the first column above correspond to cumulative spending for each of these:

```{r}
cumsum(gsProbability(k = xnb$k, n.I = xnb$n.I, theta = 0, a = rep(-20, xnb$k), b = xnb$upper$bound)$upper$prob)
# Specified spending
xnb$upper$sf(alpha = xnb$alpha, t = xnb$timing, x$upper$param)$spend
```
Again, this deviates a bit from the integer-based design:

```{r}
cumsum(gsProbability(k = xnbi$k, n.I = xnbi$n.I, theta = 0, a = rep(-20, xnbi$k), b = xnbi$upper$bound)$upper$prob)
# Specified spending
xnbi$upper$sf(alpha = xnbi$alpha, t = xnbi$timing, xnbi$upper$param)$spend
```
Finally, we look at cumulative lower boundary crossing probabilities under the alternate hypothesis for each design and compare them to the planned $\beta$-spending.
Due to rounding up the final sample size, the integer-based design has slightly higher total power than the specified 80% (Type II err0r `beta = 0.2.`
Interim power is slightly lower for the integer-based design since sample size is rounded to the nearest integer rather than rounded up as at the final analysis.
The final digit in cumulative lower boundary crossing probability is off by 1 from the targeted $\beta$-spending, presumably due to numerical integration accuracy.

```{r}
cumsum(xnb$lower$prob[,2])
xnb$lower$sf(alpha=xnb$beta, t = xnb$n.I / max(xnb$n.I), param = xnb$lower$param)$spend
```
```{r}
cumsum(xnbi$lower$prob[,2])
xnbi$lower$sf(alpha=xnbi$beta, t = xnbi$n.I / max(xnbi$n.I), param = xnbi$lower$param)$spend
```

The $\beta$-spending lower than 0.2 in the first row above is due to the final sample size powering the trial to greater than 0.8 as seen below.

```{r}
sum(xnbi$upper$prob[,2])
sum(xnbi$upper$prob[,2]) + sum(xnbi$lower$prob[,2])
```

## Futility spending under H0

```{r}
xtt6 <- gsDesign(alpha = .025, beta = .2, n.fix = n.fix, test.type = 6, 
                sfu = sfLDOF, sfl = sfHSD, sflpar = 1, 
                timing = c(.5, .75), delta1 = .1)
xtt6$n.I
x$n.I
```

```{r}
xtt6i <- toInteger(xtt6, ratio = 2)
```



# References
